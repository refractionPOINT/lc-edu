<htmL>
    <head>
        <link rel="stylesheet" href="../style.css">
    </head>
    <body>
        <div class="container">
            <iframe width="670" height="377" src="https://www.youtube.com/embed/Dtys5FaDUO0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        
            <h2>Output Modules</h2>

            <h3>Streams</h3>
            
            <p>When creating a new Output, you chose a Stream. Streams determine the type of data you want to flow through the Output.</p>
            
            <ul>
                <li><code>event</code>: this is the bulk of the events coming from sensors, this is a very verbose stream.</li>
                <li><code>detect</code>: this is for alerts, as generated by the report action in D&R rules.</li>
                <li><code>audit</code>: the audit events are meta-events generated by the various LC systems, like access control.</li>
                <li><code>deployment</code>: these are also meta-events about your deployment, like sensor enrollments and cloned sensors.</li>
                <li><code>artifact</code>: these are meta-events reporting newly ingested files through the Artifact Collection mechanism.</li>
            </ul>

            <h3>Configurations</h3>
            
            <h4>General Parameters</h4>
            
            <ul>
                <li><code>is_flat</code>: take the json output and flatten the whole thing to a flat structure.</li>
                <li><code>inv_id</code>: only send events matching the investigation id to this output (event stream only).</li>
                <li><code>tag</code>: only send events from sensors with this tag to this output (event stream only).</li>
                <li><code>cat</code>: only send detections from this category to this output (detect stream only).</li>
                <li><code>cat_black_list</code>: only send detections that do not match the prefixes in this list (newline-seperated).</li>
                <li><code>event_white_list</code>: only send event of the types in this list (newline-seperated).</li>
                <li><code>event_black_list</code>: only send event not of the types in this list (newline-seperated).</li>
                <li><code>is_delete_on_failure</code>: if an error occurs during output, delete the output automatically.</li>
            </ul>

            <h3>Amazon S3</h3>

            <p>Output events and detections to an Amazon S3 bucket.</p>
            
            <ul>
                <li><code>bucket</code>: the path to the AWS S3 bucket.</li>
                <li><code>key_id</code>: the id of the AWS auth key.</li>
                <li><code>secret_key</code>: the AWS secret key to auth with.</li>
                <li><code>sec_per_file</code>: the number of seconds after which a file is cut and uploaded.</li>
                <li><code>is_compression</code>: if set to "true", data will be gzipped before upload.</li>
                <li><code>is_indexing</code>: if set to "true", data is uploaded in a way that makes it searchable.</li>
                <li><code>region_name</code>: optionally specify a region name.</li>
                <li><code>endpoint_url</code>: optionally specify a custom endpoint URL, usually used with region_name to output to S3-compatible 3rd party services.</li>
                <li><code>dir</code>: the directory prefix.</li>
            </ul>

            <p>Example:</p>

            <script src="https://gist.github.com/tekgrunt/386b226eb7ff0250bec51f6d2c27b9ee.js"></script>

            <h3>Google Cloud Storage</h3>

            <p>Output events and detections to a GCS bucket.</p>

            <ul>
                <li><code>bucket</code>: the path to the AWS S3 bucket.</li>
                <li><code>secret_key</code>: the secret json key identifying a service account.</li>
                <li><code>sec_per_file</code>: the number of seconds after which a file is cut and uploaded.</li>
                <li><code>is_compression</code>: if set to "true", data will be gzipped before upload.</li>
                <li><code>is_indexing</code>: if set to "true", data is uploaded in a way that makes it searchable.</li>
                <li><code>dir</code>: the directory prefix where to output the files on the remote host.</li>
            </ul>

            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/97f71b576b411c4f57304002c8cf9270.js"></script>

            <h3>SCP</h3>
            
            <p>Output events and detections over SCP (SSH file transfer).</p>
            
            <ul>
                <li><code>dest_host</code>: the ip:port where to send the data to, like <code>1.2.3.4:22</code></li>
                <li><code>dir</code>: the directory where to output the files on the remote host.</li>
                <li><code>username</code>: the SSH username to log in with.</li>
                <li><code>password</code>: optional password to use to login with.</li>
                <li><code>secret_key</code>: the optional SSH private key to authenticate with.</li>
            </ul>
            
            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/0b8a98e47abe5a2cb866c38cac5897b6.js"></script>

            <h3>SFTP</h3>

            <p>Output events and detections over SFTP.</p>
            
            <ul>
                <li><code>dest_host</code>: the ip:port where to send the data to, like <code>1.2.3.4:22</code>.</li>
                <li><code>dir</code>: the directory where to output the files on the remote host.</li>
                <li><code>username</code>: the username to log in with.</li>
                <li><code>password</code>: optional password to use to login with.</li>
                <li><code>secret_key</code>: the optional SSH private key to authenticate with.</li>
            </ul>

            <p>Example:<p>
            
            <script src="https://gist.github.com/tekgrunt/b7b0ef5a9f84c4885f7bf9d9b34e847b.js"></script>

            <h3>Slack</h3>

            <p>Output detections and audit (only) to a Slack community and channel. The Slack integration currently uses Slack Legacy Tokens.</p>
            
            <ul>
            <li><code>slack_api_token</code>: the Slack provided API token used to authenticate.</li>
            <li><code>slack_channel</code>: the channel to output to in the community.</li>
            </ul>
            
            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/258d84a4bbaacdb9208c2d57c3fd442b.js"></script>
            
            <h3>Syslog (TCP)</h3>

            <p>Output events and detections to a syslog target.</p>
            
            <ul>
                <li><code>dest_host</code>: the IP or DNS and port to connect to, format www.myorg.com:514.</li>
                <li><code>is_tls</code>: if true will output over TCP/TLS.</li>
                <li><code>is_strict_tls</code>: if true will enforce validation of TLS certs.</li>
                <li><code>is_no_header</code>: if true will not emit a Syslog header before every message. This effectively turns it into a TCP output.</li>
                <li><code>structured_data</code>: arbitrary field to include in syslog "Structured Data" headers. Sometimes useful for cloud SIEMs integration.</li>
            </ul>
            
            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/bf3e5695d30c4552a1c84c5cc229a492.js"></script>
            
            <h3>Webhook</h3>

            <p>Output individually each event, detection, audit, deployment or artifact through a POST webhook.</p>
            
            <ul>
                <li><code>dest_host</code>: the IP or DNS, port and page to HTTP(S) POST to, format https://www.myorg.com:514/whatever.</li>
                <li><code>secret_key</code>: an arbitrary shared secret used to compute an HMAC (SHA256) signature of the webhook to verify authenticity. See "Webhook" section below.</li>
                <li><code>auth_header_name</code> and <code>auth_header_value</code>: set a specific value to a specific HTTP header name in the outgoing webhooks.</li>
            </ul>

            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/5821192489d04b4610737a886edf655f.js"></script>
            
            <h3>Webhook Bulk</h3>

            <p>Output batches of events, detections, audits, deployments or artifacts through a POST webhook.</p>
            
            <ul>
                <li><code>dest_host</code>: the IP or DNS, port and page to HTTP(S) POST to, format <code>https://www.myorg.com:514/whatever</code>.</li>
                <li><code>secret_key</code>: an arbitrary shared secret used to compute an HMAC (SHA256) signature of the webhook to verify authenticity. See "Webhook" section below.</li>
                <li><code>auth_header_name</code> and <code>auth_header_value</code>: set a specific value to a specific HTTP header name in the outgoing webhooks.</li>
            </ul>

            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/b6078e937aa607851bcb6ef6bf746aeb.js"></script>
            
            <h3>SMTP</h3>

            <p>Output individually each event, detection, audit, deployment or log through an email.</p>
            
            <ul>
                <li><code>dest_host</code>: the IP or DNS (and optionally port) of the SMTP server to use to send the email.</li>
                <li><code>dest_email</code>: the email address to send the email to.</li>
                <li><code>dest_email</code>: the email address to send the email to.</li>
                <li>from_email</code>: the email address to set in the From field of the email sent.</li>
                <li><code>dest_email</code>: the email address to send the email to.</li>
                <li><code>dest_email</code>: the email address to send the email to.</li>
                <li>username</code>: the username (if any) to authenticate with the SMTP server with.</li>
                <li><code>dest_email</code>: the email address to send the email to.</li>
                <li><code>password</code>: the password (if any) to authenticate with the SMTP server with.</li>
                <li><code>secret_key</code>: an arbitrary shared secret used to compute an HMAC (SHA256) signature of the email to verify authenticity. See "Webhook" section below.</li>
                <li><code>is_readable</code>: if 'true' the email format will be HTML and designed to be readable by a human instead of a machine.</li>
                <li><code>is_starttls</code>: if 'true', use the Start TLS method of securing the connection instead of pure SSL.</li>
                <li><code>is_authlogin</code>: if 'true', authenticate using <code>AUTH LOGIN</code> instead of <code>AUTH PLAIN</code>.</li>
                <li><code>subject</code>: is specified, use this as the alternate "subject" line.</li>
            </ul>

            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/f809f1f51abe685951c0c49cad61eda2.js"></script>
            
            <h3>Humio</h3>
            
            <p>Output events and detections to the Humio.com service.</p>
            
            <ul>
                <li><code>humio_repo</code>: the name of the humio repo to upload to.</li>
                <li><code>humio_api_token</code>: the humio ingestion token.</li>
                <li><code>endpoint_url</code>: optionally specify a custom endpoint URL, if you have Humio deployed on-prem use this to point to it, otherwise it defaults to the Humio cloud.</li>
            </ul>
            
            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/46a31c465e37e4b4d044aa38df0ee177.js"></script>
            
            <p><strong>Note</strong>: You may need to create a new parser in Humio to correctly parse timestamps. You can use the following JSON parser:</p>
            
            <script src="https://gist.github.com/tekgrunt/91b7206ac327f3a920c37e6a54ab7333.js"></script>

            <h3>Kafka</h3>
            
            <p>Output events and detections to a Kafka target.</p>
            
            <ul>
            <li><code>dest_host</code>: the IP or DNS and port to connect to, format kafka.myorg.com.</li>
            <li><code>is_tls</code>: if true will output over TCP/TLS.</li>
            <li><code>is_strict_tls</code>: if true will enforce validation of TLS certs.</li>
            <li><code>username</code>: if specified along with password, use for Basic authentication.</li>
            <li><code>password</code>: if specified along with username, use for Basic authentication.</li>
            <li><code>routing_topic</code>: use the element with this name from the routing of the event as the Kafka topic name.</li>
            <li><code>literal_topic</code>: use this specific value as a topic.</li>
            </ul>
            
            <p>Example:</p>
            
            <script src="https://gist.github.com/tekgrunt/b863b836e78e1dd6a7c7fb8c39e86066.js"></script>
            
            <h2>Integrations</h2>
            
            <h3>Common Patterns</h3>
            
            <p>Here are a few common topologies used with LimaCharlie Cloud (LCC).</p>
            
            <p>All data over batched files via SFTP, Splunk or ELK consumes the received files for ingestion.</p>
            
            <code>Sensor ---> LCC (All Streams) ---> SFTP ---> ( Splunk | ELK )</code>
            
            <p>All data stramed in real-time via Syslog, Splunk or ELK receive directly via an open Syslog socket.</p>
            
            <code>Sensor ---> LCC (All Streams) ---> Syslog( TCP+SSL) ---> ( Splunk | ELK )</code>
            
            <p>All data over batched files stored on Amazon S3, Splunk or ELK consumes the received files remotely for ingestion.</p>
            
            <code>Sensor ---> LCC (All Streams) ---> Amazon S3 ---> ( Splunk | ELK )</code>
            
            <p>Bulk events are uploaded to Amazon S3 for archival while alerts and auditing events are sent in real-time to Splunk via Syslog. This has the added benefit of reducing Splunk license cost while keeping the raw events available for analysis at a cheaper cost.</p>
            
            <code>Sensor ---> LCC (Event Stream) ---> Amazon S3
                   +--> LCC (Alert+Audit Streams) ---> Syslog (TCP+SSL) ---> Splunk</code>
            
            <h3>Splunk</h3>

            <p>Splunk provides you with a simple web interface to view and search the data. It has a paying enterprise version and a free tier.</p>
            
            <p>Below are manual steps to using Splunk with LimaCharlie data. But you can also use this <a href="https://doc.limacharlie.io/docs/documentation/docs/install_simple_splunk.sh" target="_blank">installation script</a> to install and configure a free version on a Debian/Ubuntu server automatically.</p>
            
            <p>Because the LimaCharlie.io cloud needs to be able to reach your Splunk instance at all times to upload data, we recommend you create a virtual machine at a cloud provider like DigitalOcean, Amazon AWS or Google Cloud.</p>
            
            <p>Splunk is the visualization tool, but there are many ways you can use to get the data to Splunk. We will use SFTP as it is fairly simple and safe.</p>
            
            <ol>
                <li>Create your virtual machine, for example using this DigitalOcean tutorial.</li>
                <li>Install Splunk, here is a quick tutotial on how to do that.</li>
                <li>Configure a write-only user and directory for SFTP using this guide.</li>
                <li>We recommend using PasswordAuthentication false and to use RSA keys instead, but for ease you may simply set a password.</li>
                <li>Edit the file <code>/opt/splunk/etc/apps/search/local/props.conf</code> and add the following lines: <code>limacharlie SHOULD_LINEMERGE = false</code></li>
                <li>Edit the file <code>/opt/splunk/etc/apps/search/local/inputs.conf</code> and add the following lines: <code>batch:///var/sftp/uploads disabled = false sourcetype = limacharlie move_policy = sinkhole</code></li>
                <li>Restart Splunk by issuing: <code>sudo /opt/splunk/bin/splunk restart</code>.</li>
                <li>Back in <code>limacharlie.io</code>, in your organization view, create a new <code>Output</code>.</li>
                <li>Give it a name, select the <code>"sftp"</code> module and select the stram you would like to send.</li>
                <li>Set the "username" that you used to setup the SFTP service.</li>
                <li>Set either the "password"field or the "secret_key" field depending on which one you chose when setting up SFTP.</li>
                <li>In "dest_host", input the public IP address of the virtual machine you created.</li>
                <li>Set the "dir" value to "/uploads/".</li>
                <li>Click "Create".</li>
                <li>After a minute, the data should start getting written to the <code>/var/sftp/uploads</code> directory on the server and Splunk should ingest it.</li>
                <li>In Splunk, doing a query for "sourcetype=limacharlie" should result in your data.</li>
            </ol>

            <p>If you are using the free version of Splunk, note that user management is not included. The suggested method to make access to your virtual machine safe is to use an SSH tunnel. This will turn a local port into the remote Splunk port over a secure connection. A sample SSH tunnel command looks like this:</p>
            
            <script src="https://gist.github.com/tekgrunt/96588cc043fcfe0b78de7c165b58a57b.js"></script>

            <p>Then you can connect through the tunnel with your browser at <code>http://127.0.0.1:8000/</code>.</p>
            
            <h3>Amazon S3</h3>
            
            <p>If you have your own visualization stack, or you just need the data archived, you can upload directly to Amazon S3. This way you don't need any infrastructure.</p>
            
            <p>If the <code>is_indexing</code> option is enabled, data uploaded to S3 will be in a specific format enabling some indexed queries. LC data files begin with a <code>d</code> while special manifest files (indicating which data files contain which sensors' data) begin with an <code>m</code>. Otherwise (not <code>is_indexing</code>) data is uploaded as flat files with a UUID name.</p>
            
            <p>The <code>is_compression</code> flag, if on, will compress each file as GZIP when uploaded.</p>
            
            <p>It is recommended you enable <code>is_compression</code>.</p>
            
            <ol>
                <li>Log in to AWS console and go to the IAM service.</li>
                <li>Click on "Users" from the menu.</li>
                <li>Click "Add User", give it a name and select "Programmatic access".</li>
                <li>Click "Next permissions", then "Next review", you will see a warning about no access, ignore it and click "Create User".</li>
                <li>Take note of the "Access key", "Secret access key" and ARN name for the user (starts with "arn:").</li>
                <li>Go to the S3 service.</li>
                <li>Click "Create Bucket", enter a name and select a region.</li>
                <li>Select "Bucket policy" and input the policy in <a href="https://doc.limacharlie.io/docs/documentation/docs/outputs.md#policy-sample" target="_blank">sample below</a>: where you replace the "<<USER_ARN>>" with the ARN name of the user you created and the "<<BUCKET_NAME>>" with the name of the bucket you just created.</li>
                <li>Click "Save".</li>
                <li>Click the "Permissions" tab for your bucket.</li>
                <li>Back in limacharlie.io, in your organization view, create a new Output.</li>
                <li>Give it a name, select the "s3" module and select the stream you would like to send.</li>
                <li>Enter the bucket name, <code>key_id</code> and <code>secret_key</code> you noted down from AWS.</li>
                <li>Click "Create".</li>
                <li>After a minute, the data should start getting written to your bucket.</li>
            </ol>

            <h4>Policy Sample</h4>
            
            <script src="https://gist.github.com/tekgrunt/a95d17f014849022f1d7671fdc678fac.js"></script>
            
            <h3>Google Cloud Storage</h3>
            
            <p>If you have your own visualization stack, or you just need the data archived, you can upload directly to Google Cloud Storage (GCS). This way you don't need any infrastructure.</p>
            
            <p>If the <code>is_indexing</code> option is enabled, data uploaded to GCS will be in a specific format enablingsome indexed queries. LC data files begin with a d while special manifest files (indicating which data files contain which sensors' data) begin with an m. Otherwise (not <code>is_indexing</code>) data is uploaded as flat files with a UUID name.</p>
            
            <p>The <code>is_compression flag</code>, if on, will compress each file as GZIP when uploaded.</p>
            
            <p>It is recommended you enable <code>is_indexing</code> and <code>is_compression</code>.</p>
            
            <ol>
                <li>Go to the <a href="https://console.cloud.google.com/iam-admin/serviceaccounts" target="_blank">IAM Service Account console</a>.</li>
                <li>Click "Create Service Account", give it a name, no role, check the "Furnish a new private key".</li>
                <li>The private key will download to your computer, this is the file containing the key you will later set as <code>secret_key</code> in the GCS Output.</li>
                <li>Go to the <a href="https://console.cloud.google.com/storage" target="_blank">Google Cloud Storage console</a>.</li>
                <li>Create a new bucket in whatever region you prefer.</li>
                <li>In your new bucket, click "Permissions", then "Add member".</li>
                <li>Enter the name of the Service Account you created above.</li>
                <li>As a role, select "Storage" --> "Storage Object Creator" and "Storage Legacy Bucket <li>Writer" (this will grant Write-Only access to this account).</li>
                <li>Back in limacharlie.io, in your organization view, create a new Output.</li>
                <li>Give it a name, select the "gcs" module and select the stream you would like to send.</li>
                <li>Enter the bucket name and secret_key (contents of the file automatically downloaded when you created the Service Account).</li>
                <li>Click "Create".</li>
                <li>After a minute, the data should start getting written to your bucket.</li>
            </ol>

            <p>Now this has created a single Service Account in Write-Only mode.</p>
            
            <h3>HTTP Streaming</h3>
            
            <p>It is also possible to stream an output over HTTPS. This interface allows you to stream smaller dataset like investigations or specific sensors or detections. This stream can be achieved via HTTP only without any additional software layer, although the Python API makes this task easier using the Spout object.</p>
            
            <p>This feature is heavily used by the Web Interface's Live view of a sensor.</p>
            
            <p>This feature is activated like this:</p>
            
            <p>Issuing an HTTP POST to <code>https://stream.limacharlie.io/<OID></code> where <code><OID></code> is the organization ID you would like to stream from. As additional data in the POST, specify the following parameters:</p>
            
            <ul>
            <li><code>api_key</code>: this is the secret API key as provided to you in limacharlie.io.</li>
            <li><code>type</code>: this is the stream type you would like to create, one of event, detect, audit, deployment or log.</li>
            <li><code>cat</code>: optional, specifies the detection type to filter on.</li>
            <li><code>tag</code>: optional, specifies the sensor tags to filter on.</li>
            <li><code>inv_id</code>: optional, specifies the investigation ID to filter on.</li>
            </ul>

            <p>The response from this POST will be a stream of data. The format of this data will be newline-seperated JSON much like all other Outputs.</p>
            
            <p>Note that this method of getting data requires you to have a fast enough connection to receive the data as the buffering done on the side of <code>stream.limacharlie.io</code> is very minimal. If you are not fast enough, data will be dropped and you will be notified of this by special events in the stream like this: <code>{"__trace":"dropped", "n":5}</code> where n is the number of that were dropped. If no data is present in the stream (like rare detections), you will also receive a <code>{"__trace":"keepalive"}</code> message aproximately every minute to indicate the stream is still alive.</p>
            
            <h3>Webhook</h3>

            <p>Using this ouput, every element will be sent over HTTP(S) to a webserver of your choice via a POST.</p>
            
            <p>The JSON data will be found in the <code>data</code> parameter of the <code>application/x-www-form-urlencoded</code> encoded POST.</p>
            
            <p>An HTTP header name <code>Lc-Signature</code> will contain an HMAC signature of the contents. This HMAC is computed from the string value of the <code>data</code> parameter and the <code>secret_key</code> set when creating the Output, using SHA256 as the hashing algorithm.</p>
            
            <p>The validity of the signature can be checked manually or using the <code>Webhook</code> objects of the <a href="https://github.com/refractionpoint/python-limacharlie/" target="_blank">Python API</a> or the <a href="https://www.npmjs.com/package/limacharlie" target="_blank">JavaScript API</a>.</p>
            
            <p>For example, here is a sample Google Cloud Function that can receive a webhook:</p>
            
            <script src="https://gist.github.com/tekgrunt/2d6a24761b5dfcf955318e162b7953b0.js"></script>

            <h3>Security Onion</h3>

            <p>A great guide for integrating LimaCharlie into <a href="https://securityonion.net/" target="_blank">Security Onion</a> is available <a href="https://medium.com/@wlambertts/security-onion-limacharlie-befe5e8e91fa" target="_blank">here</a> along with the <a href="https://github.com/weslambert/securityonion-limacharlie/" target="_blank">code here</a>.</p>
        </div>
    </body>
</htmL>